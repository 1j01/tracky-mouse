<html>

<head>
	<meta charset="UTF-8">
	<title>Head Tracker</title>

	<!-- <script src="lib/tf.js"></script> -->
	<!-- <script src="lib/facemesh/facemesh.js"></script> -->
	<script type="module">
		import workerize from './lib/workerize.js';
		console.log("HELLO");

		window.facemesh = workerize(`
			//import facemesh from './lib/facemesh';
			importScripts('${location.origin}/lib/facemesh.js');

			export async function load(...args) {
console.log("helllloooo??", ...args);
				return await facemesh.load(...args)
			};
		`);

		console.log(facemesh);
	</script>

	<script src="lib/clmtrackr.js"></script>

	<script src="lib/jsfeat-min.js"></script>

	<link rel="stylesheet" type="text/css" href="style.css">
</head>

<body>
	<h2>Point Tracking and Head Tracking (<a href="head-tracker.js">source</a>)</h2>
	<p>
		This is an experiment in implementing a head tracking system similar to <a href="https://eviacam.crea-si.com/">eViacam</a>,
		using <a href="https://en.wikipedia.org/wiki/Lucas%E2%80%93Kanade_method">Lucasâ€“Kanade optical flow</a> to track points for high accuracy,
		and face recognition as a coarse input for understanding of where to place tracking points.
	</p>
	<p>
		You can click to track details in the video, but tracking points should be added automatically if your face is detected (in green not yellow).
	</p>
	<div id="controls">
		<label>Horizontal Sensitivity <input type="range" min="0" max="100" value="25" id="sensitivity-x"></label>
		<label>Vertical Sensitivity <input type="range" min="0" max="100" value="50" id="sensitivity-y"></label>
		<!-- <label>Smoothing <input type="range" min="0" max="100" value="50" id="smoothing"></label> -->
		<label><input type="checkbox" checked id="mirror"> Mirror</label>
	</div>
	<script src="head-tracker.js" async></script>
</body>

</html>
